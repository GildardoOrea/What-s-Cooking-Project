# -*- coding: utf-8 -*-
"""What's Cooking Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n6cjWmA-SO4DFxW8Actumsf6kc2gJ457
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import zipfile
# Import the libraries we'll use below.
import numpy as np
from matplotlib import pyplot as plt
import pandas as pd
import seaborn as sns  # for nicer plots
sns.set(style="darkgrid")  # default style
import plotly.graph_objs as plotly  # for interactive plots

import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import metrics
#Import scikit-learn dataset library
from sklearn import datasets
#Import Gaussian Naive Bayes model
from sklearn.naive_bayes import GaussianNB

"""# The challenge

In this code, I will be using the "What's cooking" dataset. This dataset is divided in 3 columns(features): ID, cuisine, and ingredients. The feature called ingredients will be represented as a list including all the individual ingredients of each recipe. The cuisine will represent the labels for this list of ingredients. The challenge is to identfy the type of cuisine depending on the types of ingredients that are given

# Uploading Data

First I'm going to upload the data from my local files and save the training data from the JSON file. The testing data (test.json) will not be used as it does not have labels and cannot be used to train our data
"""

from google.colab import files
files.upload()

archive_train=zipfile.ZipFile("WIC.zip",'r')
x_train_data=pd.read_json(archive_train.read('train.json'))
x_train_data.head()

"""# Evaluating Data

Here we can see an overall description of the data. We can see that we have a great amount of italian and mexican recipes. We also can see that we have very little russian and brazilian recipes
"""

plt.style.use('ggplot')
x_train_data['cuisine'].value_counts().plot(kind='bar')

#we will concatinate the ingredients of each recipe so that we can 
#conver the data to vectors more easily
x_train_data['all_ingredients'] = x_train_data['ingredients'].map(";".join)
x_train_data.head()

#I will use count vectorizer to encode our features in the form of a matrix so 
# that we can use our models and feed them with training and testing data
cv = CountVectorizer()
X = cv.fit_transform(x_train_data['all_ingredients'].values)
# we can see with this next function the number of ingredients is 3010 and the 
# number of recepies is 39774
X.shape

# I will use label encoder to encode the labels that represent the cuisine 
# of each recipe
enc = LabelEncoder()
y = enc.fit_transform(x_train_data.cuisine)

"""Creating 70% training data and 30% testing data"""

#Split the data 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

"""# Naive Bayes

We can see that our diagonal has very high values. This indicates that the predictions made by the model are very accurate

I will use Naive Bayes as a baseline,
"""

from mlxtend.plotting import plot_confusion_matrix
#Create a Gaussian Classifier
gnb = GaussianNB()

#Train the model using the training sets
gnb.fit(X_train.toarray(), y_train)

#Predict the response for test dataset
y_pred = gnb.predict(X_test.toarray())
#print accuracy
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print("Presicion:",metrics.precision_score(y_test, y_pred, average='micro'))
conf_matrix = metrics.confusion_matrix(y_test, y_pred)

fig, ax = plot_confusion_matrix(conf_mat=conf_matrix, figsize=(10, 10), cmap=plt.cm.Blues,
                                show_absolute=False,
                                show_normed=True)
plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Naive Bayes Confusion Matrix', fontsize=18)
plt.show()

"""# Logistic Regression

We can see that with logistic regression we get a pretty good accuracy of 78%
"""

logistic = LogisticRegression(random_state=0).fit(X_train, y_train)
y_pred = logistic.predict(X_test)
logistic.score(X_test, y_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print("Presicion:",metrics.precision_score(y_test, y_pred, average='micro'))
conf_matrix = metrics.confusion_matrix(y_test, y_pred)

#We can see the confusion matrix for this model
from mlxtend.plotting import plot_confusion_matrix
 
fig, ax = plot_confusion_matrix(conf_mat=conf_matrix, figsize=(10, 10), cmap=plt.cm.Blues,
                                show_absolute=False,
                                show_normed=True)
plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Logistic Regression Confusion Matrix', fontsize=18)
plt.show()

"""# Desicion Tree"""

from sklearn import tree
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print("Presicion:",metrics.precision_score(y_test, y_pred, average='micro'))
conf_matrix = metrics.confusion_matrix(y_test, y_pred)

fig, ax = plot_confusion_matrix(conf_mat=conf_matrix, figsize=(10, 10), cmap=plt.cm.Blues,
                                show_absolute=False,
                                show_normed=True)
plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Desicion Tree Confusion Matrix', fontsize=18)
plt.show()

from sklearn.tree import DecisionTreeClassifier
for max_depth in range(1,15):
  model = DecisionTreeClassifier(criterion='entropy',max_depth=max_depth)
  model.fit(X_train, y_train)
  pred = model.predict(X_test)
fig, ax = plt.subplots(figsize=(24,8))
tree.plot_tree(model)
fig.suptitle('Decision tree with max_depth ={}'.format(max_depth), fontsize=14)
accuracy = metrics.accuracy_score(y_test, pred)
print('Max_depth = {};, accuracy = {:7.4f}'.format(max_depth,accuracy))

"""# Neural Network

I decided to use neural network because with this we can get a better understanding of our multiple levels of the data features such as our ingredients. So for this exercise I used Multi Layer Classifier since it seemed the most appropriate for the type of data I am dealing with.
"""

from sklearn.neural_network import MLPClassifier
import matplotlib.pyplot as plt
model = MLPClassifier(solver='adam', activation='relu', hidden_layer_sizes=5, random_state=1)
model.fit(X_train, y_train)
pred = model.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, pred))
print("Presicion:",metrics.precision_score(y_test, pred, average='micro'))
plt.plot(model.loss_curve_,label="train")
model.fit(X_test, y_test)
plt.plot(model.loss_curve_,label="validation")
plt.legend()

"""We can see that we get a really nice result aroung 73% much better than desicion trees and our baseline with Naive Bayes. Also on the graph we can see that our validation and train are very close to each other, so we can conclude that the model performs well overall compared to the true data."""

conf_matrix = metrics.confusion_matrix(y_test, pred)

fig, ax = plot_confusion_matrix(conf_mat=conf_matrix, figsize=(10, 10), cmap=plt.cm.Blues,
                                show_absolute=False,
                                show_normed=True)
plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Neural Network Confusion Matrix', fontsize=18)
plt.show()

"""# Error analysis

I think that most of the models could be improved by a lot if we clean the data first before using the models, there are some symbols and numbers that might be messing a little with the data, so I think that cleaning the data and then feeding it to the models would be more beneficial. But  for the most part most of the models performed nicely with the given data

# Conclusion

After looking at the accuracy, presission and confusion matrix of each model, we can conclude that the logisstic regression is the most accurate when classifing this type of data, followed by the neural network which also had a great performance.
"""